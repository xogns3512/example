{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 참고 노트북 \n",
    "    - https://excelsior-cjh.tistory.com/154?category=940399"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T07:30:45.981339Z",
     "start_time": "2022-02-16T07:30:45.966307Z"
    }
   },
   "outputs": [],
   "source": [
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist = input_data.read_data_sets(\"../data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T07:31:19.402214Z",
     "start_time": "2022-02-16T07:31:19.388772Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T07:31:21.382540Z",
     "start_time": "2022-02-16T07:31:21.364589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tensorflow._api.v1.keras.datasets.mnist' from 'C:\\\\Users\\\\user\\\\Anaconda3\\\\lib\\\\site-packages\\\\tensorflow\\\\_api\\\\v1\\\\keras\\\\datasets\\\\mnist\\\\__init__.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T07:31:47.922411Z",
     "start_time": "2022-02-16T07:31:47.565194Z"
    }
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN 로우레벨 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T07:33:11.882116Z",
     "start_time": "2022-02-16T07:33:09.175532Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#####################\n",
    "# Define Parameters #\n",
    "#####################\n",
    "element_size = 28\n",
    "time_steps = 28\n",
    "num_classes = 10\n",
    "batch_size = 128\n",
    "hidden_layer_size = 128\n",
    "\n",
    "# Where to save TensorBoard model summaries\n",
    "LOG_DIR = \"./logs/RNN_with_summaries\"\n",
    "\n",
    "# MNIST 데이터 불러오기 위한 함수 정의\n",
    "def mnist_load():\n",
    "    (train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    # Train set\n",
    "    train_x = train_x.astype('float32') / 255.\n",
    "    train_y = tf.keras.utils.to_categorical(train_y, num_classes=10)\n",
    "    # Test set\n",
    "    test_x = test_x.astype('float32') / 255.\n",
    "    test_y = tf.keras.utils.to_categorical(test_y, num_classes=10)\n",
    "    return (train_x, train_y), (test_x, test_y)\n",
    "\n",
    "# MNIST 데이터 불러오기\n",
    "(train_x, train_y), (test_x, test_y) = mnist_load()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(({\"image\": train_x}, train_y))\n",
    "dataset = dataset.shuffle(100000).repeat().batch(batch_size)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_batch = iterator.get_next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T07:33:22.124905Z",
     "start_time": "2022-02-16T07:33:22.115900Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create placeholders for inputs, labels\n",
    "_inputs = tf.placeholder(tf.float32, \n",
    "                       shape=[None, time_steps, element_size], \n",
    "                       name='inputs')\n",
    "y = tf.placeholder(tf.float32, shape=[None, num_classes], name='labels')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T07:36:56.171689Z",
     "start_time": "2022-02-16T07:36:56.164708Z"
    }
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T07:37:05.881812Z",
     "start_time": "2022-02-16T07:37:05.795949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    " \n",
    "  # Weights and bias for input and hidden layer\n",
    "with tf.name_scope('rnn_weights'):\n",
    "    with tf.name_scope('W_x'):\n",
    "        Wx = tf.Variable(tf.zeros([element_size, hidden_layer_size]))\n",
    "        variable_summaries(Wx)\n",
    "    with tf.name_scope('W_h'):\n",
    "        Wh = tf.Variable(tf.zeros([hidden_layer_size, hidden_layer_size]))\n",
    "        variable_summaries(Wh)\n",
    "    with tf.name_scope('Bias'):\n",
    "        b_rnn = tf.Variable(tf.zeros([hidden_layer_size]))\n",
    "        variable_summaries(b_rnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T07:37:38.862347Z",
     "start_time": "2022-02-16T07:37:38.858358Z"
    }
   },
   "outputs": [],
   "source": [
    "def rnn_step(previous_hidden_state, x):\n",
    "    current_hidden_state = tf.tanh(\n",
    "        tf.matmul(previous_hidden_state, Wh) + \n",
    "        tf.matmul(x, Wx) + b_rnn)\n",
    "    return current_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T07:42:47.533683Z",
     "start_time": "2022-02-16T07:42:47.482263Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# tf.scan() 함수로 입력값 처리\n",
    "# input shape: (batch_size, time_steps, element_size)\n",
    "processed_input = tf.transpose(_inputs, perm=[1, 0, 2])\n",
    "# transposed input shape: (time_steps, batch_size, element_size)\n",
    "\n",
    "initial_hidden = tf.zeros([batch_size, hidden_layer_size])\n",
    "# time_steps에 따른 상태 벡터(state vector) 구하기\n",
    "all_hidden_states = tf.scan(rnn_step, processed_input,\n",
    "                          initializer=initial_hidden, name='states')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T07:43:09.616710Z",
     "start_time": "2022-02-16T07:43:08.868423Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'T', b'Te', b'Ten', b'Tens', b'Tenso', b'Tensor', b'Tensor ',\n",
       "       b'Tensor F', b'Tensor Fl', b'Tensor Flo', b'Tensor Flow'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# tf.scan() 예제\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "elems = np.array(['T', 'e', 'n', 's', 'o', 'r', ' ', 'F', 'l', 'o', 'w'])\n",
    "scan_sum = tf.scan(lambda a, x: a + x, elems)\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(scan_sum)\n",
    "# sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T07:45:31.376292Z",
     "start_time": "2022-02-16T07:45:31.298554Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 출력에 적용할 가중치\n",
    "with tf.name_scope('linear_layer_weights') as scope:\n",
    "    with tf.name_scope('W_linear'):\n",
    "        Wl = tf.Variable(tf.truncated_normal([hidden_layer_size, num_classes], \n",
    "                                             mean=0, stddev=.01))\n",
    "        variable_summaries(Wl)\n",
    "    with tf.name_scope('Bias_linear'):\n",
    "        bl = tf.Variable(tf.truncated_normal([num_classes], \n",
    "                                             mean=0, stddev=.01))\n",
    "        variable_summaries(bl)\n",
    "\n",
    "# 상태 벡터에 linear layer 적용\n",
    "def get_linear_layer(hidden_state):\n",
    "    return tf.matmul(hidden_state, Wl) + bl\n",
    "\n",
    "with tf.name_scope('linear_layer_weights') as scope:\n",
    "    # 시간에 따라 반복하면서 모든 RNN 결과에 get_linear_layer 적용\n",
    "    all_outputs = tf.map_fn(get_linear_layer, all_hidden_states)\n",
    "    # 최종 결과 = h_28\n",
    "    output = all_outputs[-1]\n",
    "    tf.summary.histogram('outputs', output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T07:45:56.046223Z",
     "start_time": "2022-02-16T07:45:55.827561Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "with tf.name_scope('cross_entropy'):\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits_v2(logits=output, labels=y))\n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    # RMSPropOptimizer 사용\n",
    "    train_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cross_entropy)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(output, 1))\n",
    "    accuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32)))*100\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# 요약을 병합\n",
    "merged = tf.summary.merge_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T07:52:05.575467Z",
     "start_time": "2022-02-16T07:47:27.370699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1000\t MiniBatch Loss: 1.312871\t Training Acc=49.21875\n",
      "Iter: 2000\t MiniBatch Loss: 0.927880\t Training Acc=69.53125\n",
      "Iter: 3000\t MiniBatch Loss: 0.296155\t Training Acc=92.18750\n",
      "Iter: 4000\t MiniBatch Loss: 0.160103\t Training Acc=96.09375\n",
      "Iter: 5000\t MiniBatch Loss: 0.149526\t Training Acc=96.09375\n",
      "Iter: 6000\t MiniBatch Loss: 0.091280\t Training Acc=96.09375\n",
      "Iter: 7000\t MiniBatch Loss: 0.036761\t Training Acc=100.00000\n",
      "Iter: 8000\t MiniBatch Loss: 0.069959\t Training Acc=97.65625\n",
      "Iter: 9000\t MiniBatch Loss: 0.010371\t Training Acc=100.00000\n",
      "Iter: 10000\t MiniBatch Loss: 0.047497\t Training Acc=98.43750\n",
      "Test Accuracy: 96.09375\n"
     ]
    }
   ],
   "source": [
    " \n",
    "# Get a small test set\n",
    "test_data = test_x[:batch_size].reshape([-1, time_steps, element_size])\n",
    "test_label = test_y[:batch_size]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # LOG_DIR에 텐세보드에서 사용할 요약을 기록\n",
    "    train_writer = tf.summary.FileWriter(LOG_DIR + '/train', \n",
    "                                         graph=tf.get_default_graph())\n",
    "    test_writer = tf.summary.FileWriter(LOG_DIR + '/test', \n",
    "                                        graph=tf.get_default_graph())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "      \n",
    "          \n",
    "    for step in range(10000):\n",
    "        batch_x, batch_y = sess.run(next_batch)\n",
    "        summary, _ = sess.run([merged, train_step], \n",
    "                              feed_dict={_inputs:batch_x['image'], y:batch_y})   \n",
    "        # 요약 추가\n",
    "        train_writer.add_summary(summary, step)\n",
    "        \n",
    "        if (step+1) % 1000 == 0:\n",
    "            acc, loss = sess.run([accuracy, cross_entropy], \n",
    "                                 feed_dict={_inputs:batch_x['image'], y:batch_y})\n",
    "            print(\"Iter: %04d\\t\" % (step+1), \n",
    "                  \"MiniBatch Loss: {:.6f}\\t\".format(loss),\n",
    "                  \"Training Acc={:.5f}\".format(acc))\n",
    "            \n",
    "        if (step+1) % 100 == 0:\n",
    "            # MNIST 테스트 이미지에서 정확도를 계산해 요약에 추가\n",
    "            summary, acc = sess.run([merged, accuracy], \n",
    "                                    feed_dict={_inputs: test_data, \n",
    "                                               y: test_label})\n",
    "            test_writer.add_summary(summary, step)\n",
    "                  \n",
    "    test_acc = sess.run(accuracy, feed_dict={_inputs: test_data,\n",
    "                                             y: test_label})\n",
    "    print(\"Test Accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서플로 내장 RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T08:12:26.003043Z",
     "start_time": "2022-02-16T08:12:25.992975Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "element_size = 28 \n",
    "time_steps = 28\n",
    "num_classes = 10\n",
    "batch_size = 128\n",
    "hidden_layer_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T08:12:33.617752Z",
     "start_time": "2022-02-16T08:12:33.600756Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1) Create placeholders for inputs, labels\n",
    "_inputs = tf.placeholder(tf.float32, shape=[None, time_steps,\n",
    "                                          element_size], name='inputs')\n",
    "y = tf.placeholder(tf.float32, shape=[None, num_classes], name='inputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T08:14:39.650306Z",
     "start_time": "2022-02-16T08:14:39.544508Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-20-2feb95c30cb8>:3: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-20-2feb95c30cb8>:4: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    }
   ],
   "source": [
    "# 2) RNN Model\n",
    "# Tensorflow built-in functions\n",
    "rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_layer_size)\n",
    "outputs, _ = tf.nn.dynamic_rnn(rnn_cell, _inputs, dtype=tf.float32)\n",
    "\n",
    "Wl = tf.Variable(tf.truncated_normal([hidden_layer_size, num_classes],\n",
    "                                   mean=0, stddev=.01))\n",
    "bl = tf.Variable(tf.truncated_normal([num_classes], mean=0, stddev=.01))\n",
    "\n",
    "def get_linear_layer(vector):\n",
    "    return tf.matmul(vector, Wl) + bl\n",
    "\n",
    "last_rnn_output = outputs[:,-1,:]\n",
    "final_output = get_linear_layer(last_rnn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T08:14:54.849537Z",
     "start_time": "2022-02-16T08:14:54.600676Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3) Loss function\n",
    "cross_entropy = tf.reduce_mean(\n",
    "                  tf.nn.softmax_cross_entropy_with_logits_v2(logits=final_output, labels=y))\n",
    "# 4) Optimizer\n",
    "train_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cross_entropy)\n",
    "\n",
    "# 5) accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(final_output, 1))\n",
    "accuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32)))*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-02-16T08:16:43.757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1000\t MiniBatch Loss: 0.169439\t Training Acc=95.31250\n",
      "Iter: 2000\t MiniBatch Loss: 0.111478\t Training Acc=96.09375\n",
      "Iter: 3000\t MiniBatch Loss: 0.057166\t Training Acc=99.21875\n",
      "Iter: 4000\t MiniBatch Loss: 0.012139\t Training Acc=100.00000\n",
      "Iter: 5000\t MiniBatch Loss: 0.071971\t Training Acc=98.43750\n",
      "Iter: 6000\t MiniBatch Loss: 0.029778\t Training Acc=99.21875\n",
      "Iter: 7000\t MiniBatch Loss: 0.067311\t Training Acc=99.21875\n",
      "Iter: 8000\t MiniBatch Loss: 0.088630\t Training Acc=96.87500\n"
     ]
    }
   ],
   "source": [
    "# 6) Training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Get a small test set\n",
    "    test_data = test_x[:batch_size].reshape([-1, time_steps, element_size])\n",
    "    test_label = test_y[:batch_size]\n",
    "\n",
    "    for step in range(10000):\n",
    "        batch_x, batch_y = sess.run(next_batch)\n",
    "\n",
    "        sess.run(train_step, feed_dict={_inputs:batch_x['image'], \n",
    "                                        y:batch_y})          \n",
    "\n",
    "\n",
    "        if (step+1) % 1000 == 0:\n",
    "            acc, loss = sess.run([accuracy, cross_entropy], \n",
    "                                 feed_dict={_inputs:batch_x['image'], y:batch_y})\n",
    "            print(\"Iter: %04d\\t\" % (step+1), \n",
    "                  \"MiniBatch Loss: {:.6f}\\t\".format(loss),\n",
    "                  \"Training Acc={:.5f}\".format(acc))\n",
    "\n",
    "        if (step+1) % 100 == 0:\n",
    "            # MNIST 테스트 이미지에서 정확도를 계산해 요약에 추가\n",
    "            acc = sess.run(accuracy, feed_dict={_inputs: test_data, \n",
    "                                                y: test_label})\n",
    "            \n",
    "    test_acc = sess.run(accuracy, feed_dict={_inputs: test_data,\n",
    "                                             y: test_label})\n",
    "    print(\"Test Accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
